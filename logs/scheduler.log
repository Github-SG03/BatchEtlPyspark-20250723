OSError while attempting to symlink the latest log directory
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T06:33:55.067+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T06:33:55.091+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T06:33:55.459+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T06:33:55.462+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T06:33:55.476+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 575[0m
[[34m2025-07-19T06:33:55.480+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-19T06:33:55.509+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 06:33:55 +0000] [574] [INFO] Starting gunicorn 23.0.0
[2025-07-19 06:33:55 +0000] [574] [INFO] Listening at: http://[::]:8793 (574)
[2025-07-19 06:33:55 +0000] [574] [INFO] Using worker: sync
[2025-07-19 06:33:55 +0000] [576] [INFO] Booting worker with pid: 576
[2025-07-19 06:33:55 +0000] [577] [INFO] Booting worker with pid: 577
[[34m2025-07-19T06:33:56.162+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-07-19T06:33:56.173+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-07-19T06:33:56.704+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T06:33:57.488+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG ecommerce_etl is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-07-19T06:33:57.694+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2025-07-19T06:33:57.695+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T06:33:57.697+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2025-07-19T06:33:57.711+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='scheduled__2025-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T06:33:57.713+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'scheduled__2025-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T06:33:57.726+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'scheduled__2025-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T06:34:12.683+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
[[34m2025-07-19T06:34:43.922+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 579[0m
Traceback (most recent call last):
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/bin/airflow", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 417, in task_run
    _dag = get_dag(args.subdir, args.dag_id, args.read_from_db)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/cli.py", line 235, in get_dag
    dagbag = DagBag(first_path)
             ^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 579
[[34m2025-07-19T06:34:44.529+0000[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'scheduled__2025-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py']' returned non-zero exit status 1..[0m
[[34m2025-07-19T06:34:44.533+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='scheduled__2025-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T06:34:44.554+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=scheduled__2025-07-18T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 06:33:57.699276+00:00, queued_by_job_id=96, pid=None[0m
[[34m2025-07-19T06:34:44.556+0000[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2025-07-19T06:34:44.608+0000[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2025-07-19T06:34:44.689+0000[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=ecommerce_etl, task_id=run_etl, run_id=scheduled__2025-07-18T00:00:00+00:00, execution_date=20250718T000000, start_date=, end_date=20250719T063444[0m
25/07/19 06:34:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:34:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: /tmp/tmpz_iuzgjt/connection14670353896093914126.info
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:380)
	at java.base/java.nio.file.Files.createFile(Files.java:658)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
	at java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)
	at java.base/java.nio.file.Files.createTempFile(Files.java:878)
	at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)
	at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/07/19 06:34:55 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:34:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:34:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:35:03.344+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:35:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:35:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:35:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:35:45.969+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:36:27 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:36:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:36:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:36:40.114+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T06:36:46.221+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2025-07-19T06:36:46.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T06:36:46.225+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2025-07-19T06:36:46.237+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='scheduled__2025-07-18T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T06:36:46.239+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'scheduled__2025-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T06:36:46.287+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'scheduled__2025-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T06:37:11.477+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 06:37:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:37:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:37:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T06:37:28.625+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl scheduled__2025-07-18T00:00:00+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T06:37:53.450+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T06:37:54.073+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='scheduled__2025-07-18T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-07-19T06:37:54.122+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=scheduled__2025-07-18T00:00:00+00:00, map_index=-1, run_start_date=2025-07-19 06:37:29.430279+00:00, run_end_date=2025-07-19 06:37:53.085763+00:00, run_duration=23.655484, state=success, executor_state=success, try_number=2, max_tries=1, job_id=97, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 06:36:46.226635+00:00, queued_by_job_id=96, pid=1464[0m
[[34m2025-07-19T06:37:54.137+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=575) last sent a heartbeat 65.86 seconds ago! Restarting it[0m
[[34m2025-07-19T06:37:54.162+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 575. PIDs of all processes in the group: [575][0m
[[34m2025-07-19T06:37:54.173+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 575[0m
[[34m2025-07-19T06:38:09.635+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=575, status='terminated', exitcode=0, started='06:34:03') (575) terminated with exit code 0[0m
[[34m2025-07-19T06:38:09.650+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 1648[0m
[[34m2025-07-19T06:38:09.703+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19T06:38:10.341+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-07-19T06:38:10.760+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T06:38:11.340+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-18 00:00:00+00:00: scheduled__2025-07-18T00:00:00+00:00, state:running, queued_at: 2025-07-19 06:33:57.441358+00:00. externally triggered: False> successful[0m
[[34m2025-07-19T06:38:11.345+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-18 00:00:00+00:00, run_id=scheduled__2025-07-18T00:00:00+00:00, run_start_date=2025-07-19 06:33:57.579564+00:00, run_end_date=2025-07-19 06:38:11.343734+00:00, run_duration=253.76417, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-07-18 00:00:00+00:00, data_interval_end=2025-07-19 00:00:00+00:00, dag_hash=791f1aa786236f4e2e7db8d2d6858023[0m
[[34m2025-07-19T06:38:11.364+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG ecommerce_etl is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
25/07/19 06:38:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:38:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:38:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T06:38:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T06:33:58+00:00 [scheduled]>[0m
[[34m2025-07-19T06:38:32.971+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T06:38:32.973+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T06:33:58+00:00 [scheduled]>[0m
[[34m2025-07-19T06:38:32.995+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T06:33:58+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T06:38:32.998+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T06:33:58+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T06:38:33.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T06:33:58+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T06:38:49.871+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 06:38:57 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:38:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:38:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T06:39:03.215+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T06:33:58+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T06:39:21.285+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T06:39:21.713+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T06:33:58+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T06:39:21.753+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T06:33:58+00:00, map_index=-1, run_start_date=2025-07-19 06:39:03.500004+00:00, run_end_date=2025-07-19 06:39:20.999438+00:00, run_duration=17.499434, state=success, executor_state=success, try_number=1, max_tries=1, job_id=98, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 06:38:32.975779+00:00, queued_by_job_id=96, pid=2006[0m
[[34m2025-07-19T06:39:21.934+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-07-19T06:39:22.167+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T06:39:23.681+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 06:33:58+00:00: manual__2025-07-19T06:33:58+00:00, state:running, queued_at: 2025-07-19 06:33:58.049890+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T06:39:23.684+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 06:33:58+00:00, run_id=manual__2025-07-19T06:33:58+00:00, run_start_date=2025-07-19 06:38:32.776367+00:00, run_end_date=2025-07-19 06:39:23.684661+00:00, run_duration=50.908294, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-18 00:00:00+00:00, data_interval_end=2025-07-19 00:00:00+00:00, dag_hash=791f1aa786236f4e2e7db8d2d6858023[0m
25/07/19 06:39:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:39:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:39:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:39:54.473+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:40:15 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:40:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:40:25.611+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:40:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:40:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:41:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:41:07.434+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[2025-07-19T06:41:37.777+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:41:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:41:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:41:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:42:08.440+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:42:37 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:42:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:42:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:42:47.754+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:43:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:43:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:43:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:43:33.311+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:44:10 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:44:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:44:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:44:15.541+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T06:44:34.511+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 06:44:51 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:44:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:44:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:44:58.564+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:45:37 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:45:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:45:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:45:44.350+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:46:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:46:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:46:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:46:27.007+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:47:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:47:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:47:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:47:16.601+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:47:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:47:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:47:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:47:59.261+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:48:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:48:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:48:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:48:42.429+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:49:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:49:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:49:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:49:26.661+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T06:49:44.974+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 06:50:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:50:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:50:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:50:14.058+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:50:52 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:50:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:51:00.551+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:51:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:51:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:51:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:51:48.890+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:52:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:52:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:52:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:52:32.947+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:53:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:53:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:53:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:53:32.761+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[2025-07-19 06:53:49 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:53:49 +0000] [574] [INFO] Handling signal: winch
25/07/19 06:54:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:54:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:54:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:54:16.611+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[2025-07-19 06:54:44 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:44 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:45 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:45 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:46 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:48 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:48 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:48 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:49 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:49 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:54:49 +0000] [574] [INFO] Handling signal: winch
25/07/19 06:54:54 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:54:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T06:55:01.710+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-07-19T06:55:02.845+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:55:40 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:55:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:55:47.771+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:56:22 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:56:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:56:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:56:29.157+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:57:11 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:57:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:57:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:57:23.372+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[2025-07-19 06:58:07 +0000] [574] [INFO] Handling signal: winch
25/07/19 06:58:10 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:58:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19 06:58:14 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:58:14 +0000] [574] [INFO] Handling signal: winch
[2025-07-19 06:58:15 +0000] [574] [INFO] Handling signal: winch
[2025-07-19T06:58:24.300+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:59:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:59:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:59:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T06:59:15.757+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 06:59:55 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 06:59:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 06:59:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:00:02.725+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
[[34m2025-07-19T07:00:16.181+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 07:00:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:00:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:00:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:00:47.670+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 07:01:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:01:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:01:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:01:32.485+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 07:02:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:02:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:02:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:02:18.622+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 07:02:56 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:02:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:02:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:03:02.832+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 07:03:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:03:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:03:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:04:00.505+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 07:04:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:04:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:04:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19T07:04:53.727+0000] {processor.py:376} WARNING - Error when trying to pre-import module 'airflow.providers.papermill.operators.papermill' found in /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/notebook_runner_dag.py: No module named 'airflow.providers.papermill'
25/07/19 07:05:31 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:05:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:05:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T07:05:35.512+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 07:06:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:06:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 07:06:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:06:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:06:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 07:07:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:07:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:07:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 07:08:22 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:08:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:08:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 07:09:03 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:09:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:09:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 07:09:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 07:09:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 07:09:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T07:10:20.621+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-07-19T07:10:21.681+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1648. PIDs of all processes in the group: [1648][0m
[[34m2025-07-19T07:10:21.687+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1648[0m
[[34m2025-07-19T07:10:37.760+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=1648, status='terminated', exitcode=0, started='06:38:09') (1648) terminated with exit code 0[0m
[[34m2025-07-19T07:10:37.772+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1648. PIDs of all processes in the group: [][0m
[[34m2025-07-19T07:10:37.774+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1648[0m
[[34m2025-07-19T07:10:37.776+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 1648 as process group is missing.[0m
[[34m2025-07-19T07:10:37.778+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
[2025-07-19 07:10:37 +0000] [574] [INFO] Handling signal: term
[2025-07-19 07:10:37 +0000] [577] [INFO] Worker exiting (pid: 577)
[2025-07-19 07:10:37 +0000] [576] [INFO] Worker exiting (pid: 576)
[2025-07-19 07:10:37 +0000] [574] [INFO] Shutting down: Master
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T11:45:10.782+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T11:45:10.807+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T11:45:11.127+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T11:45:11.129+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T11:45:11.141+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 1055[0m
[[34m2025-07-19T11:45:11.145+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-19T11:45:11.172+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 11:45:11 +0000] [1054] [INFO] Starting gunicorn 23.0.0
[2025-07-19 11:45:11 +0000] [1054] [INFO] Listening at: http://[::]:8793 (1054)
[2025-07-19 11:45:11 +0000] [1054] [INFO] Using worker: sync
[2025-07-19 11:45:11 +0000] [1056] [INFO] Booting worker with pid: 1056
[2025-07-19 11:45:11 +0000] [1057] [INFO] Booting worker with pid: 1057
[2025-07-19T11:45:11.750+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
Process DagFileProcessor0-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 1058
[[34m2025-07-19T11:45:44.700+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T11:45:13+00:00 [scheduled]>[0m
[[34m2025-07-19T11:45:44.703+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T11:45:44.705+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T11:45:13+00:00 [scheduled]>[0m
[[34m2025-07-19T11:45:44.726+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T11:45:13+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T11:45:44.729+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T11:45:13+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T11:45:44.750+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T11:45:13+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
25/07/19 11:45:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:45:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: /tmp/tmpdyb3xr6v/connection11264217443303164176.info
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:380)
	at java.base/java.nio.file.Files.createFile(Files.java:658)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
	at java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)
	at java.base/java.nio.file.Files.createTempFile(Files.java:878)
	at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)
	at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[[34m2025-07-19T11:46:14.949+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 11:46:30 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:46:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:46:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T11:46:37.395+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T11:45:13+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T11:47:00.211+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T11:47:00.898+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T11:45:13+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T11:47:01.429+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T11:45:13+00:00, map_index=-1, run_start_date=2025-07-19 11:46:37.821934+00:00, run_end_date=2025-07-19 11:46:59.945690+00:00, run_duration=22.123756, state=success, executor_state=success, try_number=1, max_tries=1, job_id=100, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 11:45:44.708572+00:00, queued_by_job_id=99, pid=1344[0m
[[34m2025-07-19T11:47:01.464+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=1055) last sent a heartbeat 72.85 seconds ago! Restarting it[0m
[[34m2025-07-19T11:47:01.491+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1055. PIDs of all processes in the group: [1055][0m
[[34m2025-07-19T11:47:01.493+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1055[0m
[[34m2025-07-19T11:47:17.501+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=1055, status='terminated', exitcode=0, started='11:45:16') (1055) terminated with exit code 0[0m
[[34m2025-07-19T11:47:17.522+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 1536[0m
[[34m2025-07-19T11:47:17.590+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19T11:47:18.115+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/19 11:47:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:47:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:47:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T11:47:44.149+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 11:45:13+00:00: manual__2025-07-19T11:45:13+00:00, state:running, queued_at: 2025-07-19 11:45:13.715261+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T11:47:44.153+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 11:45:13+00:00, run_id=manual__2025-07-19T11:45:13+00:00, run_start_date=2025-07-19 11:45:44.562308+00:00, run_end_date=2025-07-19 11:47:44.153011+00:00, run_duration=119.590703, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-18 00:00:00+00:00, data_interval_end=2025-07-19 00:00:00+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 11:48:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:48:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:48:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:49:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:49:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:49:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:50:00 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:50:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:50:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T11:50:26.394+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 11:50:46 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:50:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:50:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:51:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:51:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:52:15 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:52:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:52:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:52:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:52:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:56:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:56:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:56:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:56:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:56:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:57:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:57:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:57:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:57:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T11:58:14.517+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 11:58:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:58:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:58:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:59:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:59:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:59:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 11:59:57 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 11:59:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 11:59:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:02:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:02:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:02:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:03:18 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:03:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:03:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:04:05 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:04:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:04:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:04:54 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:04:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:04:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:05:21.800+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:05:43 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:05:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:05:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:06:28 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:06:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:06:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:07:11 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:07:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:07:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:07:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:07:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:07:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:08:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:08:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:08:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:09:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:09:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:09:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:10:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:10:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:10:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:10:32.320+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:11:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:11:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:11:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:11:46 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:11:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:15:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:15:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:16:05 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:16:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:16:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-19 12:16:51 +0000] [1054] [ERROR] Worker (pid:1056) was sent SIGHUP!
[2025-07-19 12:16:51 +0000] [1054] [ERROR] Worker (pid:1057) was sent SIGHUP!
[2025-07-19 12:16:51 +0000] [7202] [INFO] Booting worker with pid: 7202
[2025-07-19 12:16:51 +0000] [7203] [INFO] Booting worker with pid: 7203
[2025-07-19 12:16:51 +0000] [1054] [INFO] Handling signal: hup
[2025-07-19 12:16:51 +0000] [1054] [INFO] Hang up: Master
[2025-07-19 12:16:51 +0000] [7203] [INFO] Worker exiting (pid: 7203)
[2025-07-19 12:16:51 +0000] [7202] [INFO] Worker exiting (pid: 7202)
[2025-07-19 12:16:51 +0000] [1054] [INFO] Handling signal: hup
[2025-07-19 12:16:51 +0000] [1054] [INFO] Hang up: Master
[2025-07-19 12:16:51 +0000] [7206] [INFO] Booting worker with pid: 7206
[2025-07-19 12:16:51 +0000] [7207] [INFO] Booting worker with pid: 7207
[2025-07-19 12:16:52 +0000] [7207] [INFO] Worker exiting (pid: 7207)
[2025-07-19 12:16:52 +0000] [7206] [INFO] Worker exiting (pid: 7206)
[2025-07-19 12:16:52 +0000] [7208] [INFO] Booting worker with pid: 7208
[2025-07-19 12:16:52 +0000] [7209] [INFO] Booting worker with pid: 7209
25/07/19 12:16:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:16:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:16:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T12:19:50.934+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T12:19:50.941+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T12:19:51.344+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T12:19:51.348+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T12:19:51.369+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 480[0m
[[34m2025-07-19T12:19:51.376+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-07-19 12:19:51 +0000] [479] [INFO] Starting gunicorn 23.0.0
[[34m2025-07-19T12:19:51.434+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 12:19:51 +0000] [479] [INFO] Listening at: http://[::]:8793 (479)
[2025-07-19 12:19:51 +0000] [479] [INFO] Using worker: sync
[2025-07-19 12:19:51 +0000] [481] [INFO] Booting worker with pid: 481
[2025-07-19 12:19:51 +0000] [482] [INFO] Booting worker with pid: 482
[[34m2025-07-19T12:19:51.921+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-07-19T12:19:52.218+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/19 12:20:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:20:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:20:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:20:13.728+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:19:54+00:00 [scheduled]>[0m
[[34m2025-07-19T12:20:13.730+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T12:20:13.732+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:19:54+00:00 [scheduled]>[0m
[[34m2025-07-19T12:20:13.747+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:19:54+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T12:20:13.749+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:19:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:20:13.764+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:19:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:20:35.778+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 12:20:44 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:20:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:20:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:20:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T12:20:49.700+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:19:54+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T12:21:09.066+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T12:21:09.517+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:19:54+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T12:21:09.703+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T12:19:54+00:00, map_index=-1, run_start_date=2025-07-19 12:20:50.083290+00:00, run_end_date=2025-07-19 12:21:08.876865+00:00, run_duration=18.793575, state=success, executor_state=success, try_number=1, max_tries=1, job_id=102, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 12:20:13.734403+00:00, queued_by_job_id=101, pid=1010[0m
[[34m2025-07-19T12:21:09.740+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=480) last sent a heartbeat 55.69 seconds ago! Restarting it[0m
[[34m2025-07-19T12:21:09.748+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 480. PIDs of all processes in the group: [480][0m
[[34m2025-07-19T12:21:09.755+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 480[0m
[[34m2025-07-19T12:21:21.986+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=480, status='terminated', exitcode=0, started='12:19:52') (480) terminated with exit code 0[0m
[[34m2025-07-19T12:21:21.993+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 1214[0m
[[34m2025-07-19T12:21:22.028+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19T12:21:22.453+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/19 12:21:36 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:21:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:21:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:21:43.293+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 12:19:54+00:00: manual__2025-07-19T12:19:54+00:00, state:running, queued_at: 2025-07-19 12:19:54.368023+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T12:21:43.297+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 12:19:54+00:00, run_id=manual__2025-07-19T12:19:54+00:00, run_start_date=2025-07-19 12:20:13.457272+00:00, run_end_date=2025-07-19 12:21:43.296874+00:00, run_duration=89.839602, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 12:19:54+00:00, data_interval_end=2025-07-19 12:19:54+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 12:22:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:22:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:22:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:23:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:23:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:23:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:24:07 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:24:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:24:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:24:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:24:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:25:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:25:09.639+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:25:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:25:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:25:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:26:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:26:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:26:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:27:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:27:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:27:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:27:50 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:27:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:27:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:28:24.174+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-07-19T12:28:25.179+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1214. PIDs of all processes in the group: [1214][0m
[[34m2025-07-19T12:28:25.181+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1214[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T12:28:51.687+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T12:28:51.692+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T12:28:52.010+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T12:28:52.013+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T12:28:52.026+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 3098[0m
[[34m2025-07-19T12:28:52.031+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-19T12:28:52.059+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 12:28:52 +0000] [3097] [INFO] Starting gunicorn 23.0.0
[2025-07-19 12:28:52 +0000] [3097] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:28:52 +0000] [3097] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[[34m2025-07-19T12:28:52.395+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-07-19T12:28:52.670+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-07-19 12:28:53 +0000] [3097] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:28:53 +0000] [3097] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[[34m2025-07-19T12:28:53.167+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=1214, status='terminated', exitcode=0, started='12:21:20') (1214) terminated with exit code 0[0m
[[34m2025-07-19T12:28:53.189+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1214. PIDs of all processes in the group: [][0m
[[34m2025-07-19T12:28:53.192+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1214[0m
[[34m2025-07-19T12:28:53.194+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 1214 as process group is missing.[0m
[[34m2025-07-19T12:28:53.197+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
[2025-07-19 12:28:53 +0000] [479] [INFO] Handling signal: term
[2025-07-19 12:28:53 +0000] [481] [INFO] Worker exiting (pid: 481)
[2025-07-19 12:28:53 +0000] [482] [INFO] Worker exiting (pid: 482)
[2025-07-19 12:28:53 +0000] [479] [INFO] Shutting down: Master
[2025-07-19 12:28:54 +0000] [3097] [INFO] Listening at: http://[::]:8793 (3097)
[2025-07-19 12:28:54 +0000] [3097] [INFO] Using worker: sync
[2025-07-19 12:28:54 +0000] [3100] [INFO] Booting worker with pid: 3100
[2025-07-19 12:28:54 +0000] [3101] [INFO] Booting worker with pid: 3101
25/07/19 12:29:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:29:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:29:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:29:13.748+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:28:56+00:00 [scheduled]>[0m
[[34m2025-07-19T12:29:13.749+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T12:29:13.751+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:28:56+00:00 [scheduled]>[0m
[[34m2025-07-19T12:29:13.766+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:28:56+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T12:29:13.769+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:28:56+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:29:13.782+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:28:56+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:29:31.999+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 12:29:43 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:29:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:29:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T12:29:46.842+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:28:56+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T12:30:01.104+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T12:30:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:28:56+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T12:30:01.438+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T12:28:56+00:00, map_index=-1, run_start_date=2025-07-19 12:29:47.281741+00:00, run_end_date=2025-07-19 12:30:00.961848+00:00, run_duration=13.680107, state=success, executor_state=success, try_number=1, max_tries=1, job_id=104, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 12:29:13.753138+00:00, queued_by_job_id=103, pid=3472[0m
[[34m2025-07-19T12:30:15.623+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 12:28:56+00:00: manual__2025-07-19T12:28:56+00:00, state:running, queued_at: 2025-07-19 12:28:56.684902+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T12:30:15.627+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 12:28:56+00:00, run_id=manual__2025-07-19T12:28:56+00:00, run_start_date=2025-07-19 12:29:13.622664+00:00, run_end_date=2025-07-19 12:30:15.627451+00:00, run_duration=62.004787, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 12:28:56+00:00, data_interval_end=2025-07-19 12:28:56+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 12:30:23 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:30:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:30:27.101+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:28:57+00:00 [scheduled]>[0m
[[34m2025-07-19T12:30:27.102+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T12:30:27.103+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:28:57+00:00 [scheduled]>[0m
[[34m2025-07-19T12:30:27.111+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:28:57+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T12:30:27.112+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:28:57+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:30:27.121+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:28:57+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:30:40.071+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 12:30:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:30:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:30:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T12:30:53.039+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:28:57+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T12:31:05.622+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T12:31:05.907+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:28:57+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T12:31:05.926+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T12:28:57+00:00, map_index=-1, run_start_date=2025-07-19 12:30:53.302444+00:00, run_end_date=2025-07-19 12:31:05.450021+00:00, run_duration=12.147577, state=success, executor_state=success, try_number=1, max_tries=1, job_id=105, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 12:30:27.104248+00:00, queued_by_job_id=103, pid=4211[0m
25/07/19 12:31:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:31:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:31:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:31:20.951+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 12:28:57+00:00: manual__2025-07-19T12:28:57+00:00, state:running, queued_at: 2025-07-19 12:29:27.922849+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T12:31:20.953+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 12:28:57+00:00, run_id=manual__2025-07-19T12:28:57+00:00, run_start_date=2025-07-19 12:30:27.013535+00:00, run_end_date=2025-07-19 12:31:20.953131+00:00, run_duration=53.939596, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 12:28:57+00:00, data_interval_end=2025-07-19 12:28:57+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 12:32:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:32:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:32:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:32:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:32:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:32:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:33:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:33:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:33:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:34:13 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:34:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:34:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:34:19.603+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:34:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:34:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:34:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:35:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:35:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:35:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:36:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:36:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:36:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:37:04 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:37:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:37:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:37:46 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:37:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:37:56.870+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-07-19T12:37:57.886+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 3098. PIDs of all processes in the group: [3098][0m
[[34m2025-07-19T12:37:57.888+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 3098[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T12:38:25.475+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T12:38:25.481+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T12:38:25.867+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T12:38:25.870+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T12:38:25.886+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 6357[0m
[[34m2025-07-19T12:38:25.890+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-19T12:38:25.916+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 12:38:25 +0000] [6356] [INFO] Starting gunicorn 23.0.0
[2025-07-19 12:38:25 +0000] [6356] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:38:25 +0000] [6356] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[[34m2025-07-19T12:38:26.228+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-07-19T12:38:26.504+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-07-19T12:38:26.818+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=3098, status='terminated', exitcode=0, started='12:29:16') (3098) terminated with exit code 0[0m
[[34m2025-07-19T12:38:26.837+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 3098. PIDs of all processes in the group: [][0m
[[34m2025-07-19T12:38:26.839+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 3098[0m
[[34m2025-07-19T12:38:26.841+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 3098 as process group is missing.[0m
[[34m2025-07-19T12:38:26.843+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
[2025-07-19 12:38:26 +0000] [3097] [INFO] Handling signal: term
[2025-07-19 12:38:26 +0000] [6356] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:38:26 +0000] [6356] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[2025-07-19 12:38:26 +0000] [3101] [INFO] Worker exiting (pid: 3101)
[2025-07-19 12:38:26 +0000] [3100] [INFO] Worker exiting (pid: 3100)
[2025-07-19 12:38:27 +0000] [3097] [INFO] Shutting down: Master
[2025-07-19 12:38:27 +0000] [6356] [INFO] Listening at: http://[::]:8793 (6356)
[2025-07-19 12:38:27 +0000] [6356] [INFO] Using worker: sync
[2025-07-19 12:38:27 +0000] [6359] [INFO] Booting worker with pid: 6359
[2025-07-19 12:38:28 +0000] [6360] [INFO] Booting worker with pid: 6360
25/07/19 12:38:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:38:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:38:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:38:48.052+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:38:30+00:00 [scheduled]>[0m
[[34m2025-07-19T12:38:48.054+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T12:38:48.056+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:38:30+00:00 [scheduled]>[0m
[[34m2025-07-19T12:38:48.078+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:38:30+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T12:38:48.080+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:38:30+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:38:48.102+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:38:30+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:39:06.920+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 12:39:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:39:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:39:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T12:39:22.568+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:38:30+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T12:39:37.096+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T12:39:37.430+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:38:30+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T12:39:37.474+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T12:38:30+00:00, map_index=-1, run_start_date=2025-07-19 12:39:23.001494+00:00, run_end_date=2025-07-19 12:39:36.915266+00:00, run_duration=13.913772, state=success, executor_state=success, try_number=1, max_tries=1, job_id=107, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 12:38:48.058987+00:00, queued_by_job_id=106, pid=6769[0m
[[34m2025-07-19T12:39:50.520+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 12:38:30+00:00: manual__2025-07-19T12:38:30+00:00, state:running, queued_at: 2025-07-19 12:38:30.451605+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T12:39:50.524+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 12:38:30+00:00, run_id=manual__2025-07-19T12:38:30+00:00, run_start_date=2025-07-19 12:38:47.911702+00:00, run_end_date=2025-07-19 12:39:50.524539+00:00, run_duration=62.612837, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 12:38:30+00:00, data_interval_end=2025-07-19 12:38:30+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 12:39:57 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:39:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:39:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:40:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:40:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:40:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:41:20 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:41:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:41:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:41:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:41:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:42:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:42:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:42:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:43:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:43:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:43:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:43:40.189+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:44:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:44:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:44:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:44:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:44:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:44:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:45:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:45:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:45:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:46:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:46:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:46:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:46:20.762+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-07-19T12:46:21.774+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 6357. PIDs of all processes in the group: [6357][0m
[[34m2025-07-19T12:46:21.776+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 6357[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T12:46:49.187+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T12:46:49.195+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T12:46:49.575+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T12:46:49.578+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T12:46:49.595+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 8967[0m
[[34m2025-07-19T12:46:49.599+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-19T12:46:49.628+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 12:46:49 +0000] [8966] [INFO] Starting gunicorn 23.0.0
[2025-07-19 12:46:49 +0000] [8966] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:46:49 +0000] [8966] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[2025-07-19T12:46:50.288+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-07-19 12:46:50 +0000] [8966] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:46:50 +0000] [8966] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[2025-07-19 12:46:51 +0000] [8966] [ERROR] Connection in use: ('::', 8793)
[2025-07-19 12:46:51 +0000] [8966] [ERROR] connection to ('::', 8793) failed: [Errno 98] Address already in use
[[34m2025-07-19T12:46:52.330+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=6357, status='terminated', exitcode=0, started='12:38:47') (6357) terminated with exit code 0[0m
[[34m2025-07-19T12:46:52.374+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 6357. PIDs of all processes in the group: [][0m
[[34m2025-07-19T12:46:52.376+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 6357[0m
[[34m2025-07-19T12:46:52.379+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 6357 as process group is missing.[0m
[[34m2025-07-19T12:46:52.381+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
[2025-07-19 12:46:52 +0000] [6356] [INFO] Handling signal: term
[2025-07-19 12:46:52 +0000] [6359] [INFO] Worker exiting (pid: 6359)
[2025-07-19 12:46:52 +0000] [6360] [INFO] Worker exiting (pid: 6360)
[2025-07-19 12:46:52 +0000] [6356] [INFO] Shutting down: Master
[2025-07-19 12:46:52 +0000] [8966] [INFO] Listening at: http://[::]:8793 (8966)
[2025-07-19 12:46:52 +0000] [8966] [INFO] Using worker: sync
[2025-07-19 12:46:52 +0000] [8989] [INFO] Booting worker with pid: 8989
[2025-07-19 12:46:52 +0000] [8990] [INFO] Booting worker with pid: 8990
25/07/19 12:47:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:47:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:47:12.008+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:46:51+00:00 [scheduled]>[0m
[[34m2025-07-19T12:47:12.011+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T12:47:12.013+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:46:51+00:00 [scheduled]>[0m
[[34m2025-07-19T12:47:12.034+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:46:51+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T12:47:12.037+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:46:51+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:47:12.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T12:46:51+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:47:33.771+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 12:47:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:47:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:47:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T12:47:53.805+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T12:46:51+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T12:48:09.948+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T12:48:10.441+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T12:46:51+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T12:48:10.609+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T12:46:51+00:00, map_index=-1, run_start_date=2025-07-19 12:47:54.397921+00:00, run_end_date=2025-07-19 12:48:09.780657+00:00, run_duration=15.382736, state=success, executor_state=success, try_number=1, max_tries=1, job_id=109, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 12:47:12.016133+00:00, queued_by_job_id=108, pid=9369[0m
[[34m2025-07-19T12:48:10.630+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=8967) last sent a heartbeat 57.51 seconds ago! Restarting it[0m
[[34m2025-07-19T12:48:10.644+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 8967. PIDs of all processes in the group: [8967][0m
[[34m2025-07-19T12:48:10.647+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 8967[0m
[[34m2025-07-19T12:48:25.818+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=8967, status='terminated', exitcode=0, started='12:46:52') (8967) terminated with exit code 0[0m
[[34m2025-07-19T12:48:25.830+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 9729[0m
[[34m2025-07-19T12:48:25.881+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19T12:48:26.395+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/19 12:48:37 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:48:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:48:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:48:41.041+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 12:46:51+00:00: manual__2025-07-19T12:46:51+00:00, state:running, queued_at: 2025-07-19 12:46:52.002526+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T12:48:41.045+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 12:46:51+00:00, run_id=manual__2025-07-19T12:46:51+00:00, run_start_date=2025-07-19 12:47:11.841723+00:00, run_end_date=2025-07-19 12:48:41.044748+00:00, run_duration=89.203025, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 12:46:51+00:00, data_interval_end=2025-07-19 12:46:51+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
[[34m2025-07-19T12:48:59.234+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-19T12:46:52_0badcc26 [scheduled]>[0m
[[34m2025-07-19T12:48:59.236+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T12:48:59.237+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-19T12:46:52_0badcc26 [scheduled]>[0m
[[34m2025-07-19T12:48:59.248+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-19T12:46:52_0badcc26', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T12:48:59.249+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-19T12:46:52_0badcc26', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:48:59.259+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-19T12:46:52_0badcc26', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T12:49:14.989+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 12:49:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:49:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:49:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T12:49:30.141+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl auto__2025-07-19T12:46:52_0badcc26 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T12:49:41.623+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T12:49:41.880+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-19T12:46:52_0badcc26', try_number=1, map_index=-1)[0m
[[34m2025-07-19T12:49:41.896+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=auto__2025-07-19T12:46:52_0badcc26, map_index=-1, run_start_date=2025-07-19 12:49:30.377173+00:00, run_end_date=2025-07-19 12:49:41.516432+00:00, run_duration=11.139259, state=success, executor_state=success, try_number=1, max_tries=1, job_id=110, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 12:48:59.238479+00:00, queued_by_job_id=108, pid=10129[0m
25/07/19 12:49:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:49:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:49:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:49:52.768+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 12:46:52+00:00: auto__2025-07-19T12:46:52_0badcc26, state:running, queued_at: 2025-07-19 12:47:26.068994+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T12:49:52.769+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 12:46:52+00:00, run_id=auto__2025-07-19T12:46:52_0badcc26, run_start_date=2025-07-19 12:48:59.159440+00:00, run_end_date=2025-07-19 12:49:52.769475+00:00, run_duration=53.610035, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 12:46:52+00:00, data_interval_end=2025-07-19 12:46:52+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 12:50:31 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:50:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:50:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:51:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:51:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:51:55 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:51:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:51:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:52:05.548+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:52:37 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:52:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:52:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:53:23 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:53:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:53:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:54:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:54:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:54:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:54:46 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:54:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:54:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:55:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:55:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:55:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:56:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:56:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:56:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:56:52 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:56:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:56:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T12:57:23.841+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 12:57:31 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:57:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:57:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:58:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:58:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:58:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:58:56 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:58:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:58:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 12:59:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 12:59:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 12:59:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:00:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:00:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:00:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:00:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:00:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:01:40 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:01:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:01:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:02:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:02:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:02:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:02:39.465+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:03:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:03:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:03:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:03:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:03:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:03:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:04:31 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:04:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:04:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:05:13 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:05:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:05:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:05:54 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:05:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:05:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:06:37 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:06:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:06:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:07:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:07:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:07:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:08:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:08:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:08:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:08:06.801+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:08:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:08:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:08:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:09:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:09:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:09:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:10:10 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:10:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:10:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:10:50 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:10:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:10:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:11:37 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:11:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:11:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:12:18 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:12:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:12:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:13:00 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:13:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:13:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:13:22.422+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:13:44 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:13:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:13:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:14:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:14:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:14:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:15:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:15:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:15:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:15:52 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:15:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:15:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:16:36 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:16:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:16:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:17:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:17:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:17:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:18:00 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:18:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:18:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:18:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:18:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:18:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:18:46.670+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:19:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:19:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:19:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:20:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:20:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:20:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:20:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:20:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:20:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:21:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:21:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:22:11 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:22:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:22:57 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:22:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:22:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:23:38 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:23:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:23:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:24:04.835+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:24:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:24:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:24:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:24:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:24:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:25:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:25:43 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:25:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:25:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:26:22 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:26:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:26:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:27:04 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:27:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:27:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:27:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:27:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:28:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:28:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:28:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:29:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:29:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:29:21.245+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:29:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:29:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:29:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:30:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:30:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:30:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:31:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:31:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:31:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:31:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:31:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:32:33 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:32:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:32:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:33:13 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:33:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:33:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:33:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:33:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:33:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:34:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:34:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:34:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:34:37.714+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:35:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:35:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:35:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:35:57 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:35:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:35:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:36:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:36:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:37:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:37:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:37:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:38:03 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:38:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:38:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:38:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:38:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:38:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:39:30 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:39:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:39:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:39:56.005+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:40:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:40:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:40:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:40:15.060+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-07-19T13:40:16.085+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 9729. PIDs of all processes in the group: [23316, 23317, 9729][0m
[[34m2025-07-19T13:40:16.088+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 9729[0m
Process DagFileProcessor130-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 203, in __init__
    self._do_init(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 330, in _do_init
    root_dir = SparkFiles.getRootDirectory()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/files.py", line 148, in getRootDirectory
    return cls._sc._jvm.org.apache.spark.SparkFiles.getRootDirectory()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py", line 1535, in __getattr__
    answer = self._gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py", line 453, in _exit_gracefully
    self.log.debug("Current Stacktrace is: %s", "\n".join(map(str, inspect.stack())))
                                                                   ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1770, in stack
    return getouterframes(sys._getframe(1), context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1745, in getouterframes
    traceback_info = getframeinfo(frame, context)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1707, in getframeinfo
    lines, lnum = findsource(frame)
                  ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1083, in findsource
    module = getmodule(object, file)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1000, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py", line 453, in _exit_gracefully
    self.log.debug("Current Stacktrace is: %s", "\n".join(map(str, inspect.stack())))
                                                                   ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1770, in stack
    return getouterframes(sys._getframe(1), context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1745, in getouterframes
    traceback_info = getframeinfo(frame, context)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1707, in getframeinfo
    lines, lnum = findsource(frame)
                  ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1083, in findsource
    module = getmodule(object, file)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1009, in getmodule
    os.path.realpath(f)] = module.__name__
    ^^^^^^^^^^^^^^^^^^^
  File "<frozen posixpath>", line 435, in realpath
  File "<frozen posixpath>", line 479, in _joinrealpath
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/manager.py", line 453, in _exit_gracefully
    self.log.debug("Current Stacktrace is: %s", "\n".join(map(str, inspect.stack())))
                                                                   ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1770, in stack
    return getouterframes(sys._getframe(1), context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1745, in getouterframes
    traceback_info = getframeinfo(frame, context)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1703, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
               ^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 963, in getsourcefile
    module = getmodule(object, filename)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/inspect.py", line 1009, in getmodule
    os.path.realpath(f)] = module.__name__
    ^^^^^^^^^^^^^^^^^^^
  File "<frozen posixpath>", line 435, in realpath
  File "<frozen posixpath>", line 479, in _joinrealpath
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 23316
[2025-07-19T13:40:38.421+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2025-07-19T13:40:38.437+0000] {process_utils.py:263} INFO - Waiting up to 5 seconds for processes to exit...
[[34m2025-07-19T13:40:38.472+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=23316, status='terminated', started='13:37:22') (23316) terminated with exit code None[0m
[[34m2025-07-19T13:40:38.484+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=9729, status='terminated', exitcode=0, started='12:48:24') (9729) terminated with exit code 0[0m
[[34m2025-07-19T13:40:38.487+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=23317, status='terminated', started='13:37:23') (23317) terminated with exit code None[0m
[[34m2025-07-19T13:40:38.493+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 9729. PIDs of all processes in the group: [][0m
[[34m2025-07-19T13:40:38.496+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 9729[0m
[[34m2025-07-19T13:40:38.498+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 9729 as process group is missing.[0m
[[34m2025-07-19T13:40:38.500+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
[2025-07-19 13:40:38 +0000] [8966] [INFO] Handling signal: term
[2025-07-19 13:40:38 +0000] [8990] [INFO] Worker exiting (pid: 8990)
[2025-07-19 13:40:38 +0000] [8989] [INFO] Worker exiting (pid: 8989)
[2025-07-19 13:40:38 +0000] [8966] [INFO] Shutting down: Master
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T13:40:43.903+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T13:40:43.912+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T13:40:44.337+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T13:40:44.355+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[2025-07-19 13:40:44 +0000] [23518] [INFO] Starting gunicorn 23.0.0
[[34m2025-07-19T13:40:44.402+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 23519[0m
[[34m2025-07-19T13:40:44.418+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-07-19 13:40:44 +0000] [23518] [INFO] Listening at: http://[::]:8793 (23518)
[2025-07-19 13:40:44 +0000] [23518] [INFO] Using worker: sync
[2025-07-19 13:40:44 +0000] [23520] [INFO] Booting worker with pid: 23520
[[34m2025-07-19T13:40:44.443+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 13:40:44 +0000] [23521] [INFO] Booting worker with pid: 23521
[2025-07-19T13:40:45.091+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/19 13:41:00 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:41:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:41:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:41:05.270+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T13:40:48+00:00 [scheduled]>[0m
[[34m2025-07-19T13:41:05.272+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T13:41:05.273+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T13:40:48+00:00 [scheduled]>[0m
[[34m2025-07-19T13:41:05.289+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T13:40:48+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T13:41:05.291+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T13:40:48+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T13:41:05.310+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T13:40:48+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T13:41:20.737+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 13:41:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:41:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T13:41:35.709+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T13:40:48+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T13:41:49.446+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T13:41:50.182+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T13:40:48+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T13:41:50.411+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T13:40:48+00:00, map_index=-1, run_start_date=2025-07-19 13:41:35.986714+00:00, run_end_date=2025-07-19 13:41:49.253501+00:00, run_duration=13.266787, state=success, executor_state=success, try_number=1, max_tries=1, job_id=112, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 13:41:05.275048+00:00, queued_by_job_id=111, pid=23911[0m
25/07/19 13:42:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:42:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:42:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:42:07.457+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 13:40:48+00:00: manual__2025-07-19T13:40:48+00:00, state:running, queued_at: 2025-07-19 13:40:48.100506+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T13:42:07.462+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 13:40:48+00:00, run_id=manual__2025-07-19T13:40:48+00:00, run_start_date=2025-07-19 13:41:05.153921+00:00, run_end_date=2025-07-19 13:42:07.462055+00:00, run_duration=62.308134, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 13:40:48+00:00, data_interval_end=2025-07-19 13:40:48+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 13:42:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:42:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:42:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:43:28.687+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2025-07-19T13:43:29.708+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 23519. PIDs of all processes in the group: [24577, 24593, 23519][0m
[[34m2025-07-19T13:43:29.712+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 23519[0m
[2025-07-19T13:43:42.928+0000] {process_utils.py:263} INFO - Waiting up to 5 seconds for processes to exit...
[[34m2025-07-19T13:43:42.962+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=23519, status='terminated', exitcode=0, started='13:40:52') (23519) terminated with exit code 0[0m
[[34m2025-07-19T13:43:42.968+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=24593, status='terminated', started='13:43:22') (24593) terminated with exit code None[0m
[[34m2025-07-19T13:43:42.970+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=24577, status='terminated', started='13:43:20') (24577) terminated with exit code None[0m
[[34m2025-07-19T13:43:42.976+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 23519. PIDs of all processes in the group: [][0m
[[34m2025-07-19T13:43:42.978+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 23519[0m
[[34m2025-07-19T13:43:42.980+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 23519 as process group is missing.[0m
[[34m2025-07-19T13:43:42.981+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
[2025-07-19 13:43:43 +0000] [23518] [INFO] Handling signal: term
[2025-07-19 13:43:43 +0000] [23520] [INFO] Worker exiting (pid: 23520)
[2025-07-19 13:43:43 +0000] [23521] [INFO] Worker exiting (pid: 23521)
[2025-07-19 13:43:43 +0000] [23518] [INFO] Shutting down: Master
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-19T13:43:53.576+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-19T13:43:53.580+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-19T13:43:53.837+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-19T13:43:53.840+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-19T13:43:53.852+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 24673[0m
[[34m2025-07-19T13:43:53.856+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-19T13:43:53.881+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-19 13:43:53 +0000] [24672] [INFO] Starting gunicorn 23.0.0
[2025-07-19 13:43:53 +0000] [24672] [INFO] Listening at: http://[::]:8793 (24672)
[2025-07-19 13:43:53 +0000] [24672] [INFO] Using worker: sync
[2025-07-19 13:43:53 +0000] [24674] [INFO] Booting worker with pid: 24674
[2025-07-19 13:43:53 +0000] [24675] [INFO] Booting worker with pid: 24675
[2025-07-19T13:43:54.337+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/19 13:44:05 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:44:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:44:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:44:09.328+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T13:43:54+00:00 [scheduled]>[0m
[[34m2025-07-19T13:44:09.330+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T13:44:09.331+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T13:43:54+00:00 [scheduled]>[0m
[[34m2025-07-19T13:44:09.348+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T13:43:54+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T13:44:09.350+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T13:43:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T13:44:09.367+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-19T13:43:54+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T13:44:26.453+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 13:44:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:44:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:44:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T13:44:39.058+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-19T13:43:54+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T13:44:52.066+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T13:44:52.341+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-19T13:43:54+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-19T13:44:52.360+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-19T13:43:54+00:00, map_index=-1, run_start_date=2025-07-19 13:44:39.341200+00:00, run_end_date=2025-07-19 13:44:51.922419+00:00, run_duration=12.581219, state=success, executor_state=success, try_number=1, max_tries=1, job_id=114, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 13:44:09.333689+00:00, queued_by_job_id=113, pid=25092[0m
25/07/19 13:45:03 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:45:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:45:07.461+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 13:43:54+00:00: manual__2025-07-19T13:43:54+00:00, state:running, queued_at: 2025-07-19 13:43:54.944267+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T13:45:07.463+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 13:43:54+00:00, run_id=manual__2025-07-19T13:43:54+00:00, run_start_date=2025-07-19 13:44:09.190936+00:00, run_end_date=2025-07-19 13:45:07.463131+00:00, run_duration=58.272195, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 13:43:54+00:00, data_interval_end=2025-07-19 13:43:54+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
[[34m2025-07-19T13:45:08.871+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-19T13:43:55_f143e9e6 [scheduled]>[0m
[[34m2025-07-19T13:45:08.873+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-19T13:45:08.874+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-19T13:43:55_f143e9e6 [scheduled]>[0m
[[34m2025-07-19T13:45:08.888+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-19T13:43:55_f143e9e6', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-19T13:45:08.890+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-19T13:43:55_f143e9e6', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T13:45:08.907+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-19T13:43:55_f143e9e6', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-19T13:45:27.380+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/19 13:45:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:45:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:45:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-19T13:45:39.294+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl auto__2025-07-19T13:43:55_f143e9e6 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-19T13:45:53.191+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-19T13:45:53.448+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-19T13:43:55_f143e9e6', try_number=1, map_index=-1)[0m
[[34m2025-07-19T13:45:53.464+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=auto__2025-07-19T13:43:55_f143e9e6, map_index=-1, run_start_date=2025-07-19 13:45:39.631923+00:00, run_end_date=2025-07-19 13:45:53.030162+00:00, run_duration=13.398239, state=success, executor_state=success, try_number=1, max_tries=1, job_id=115, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-19 13:45:08.876221+00:00, queued_by_job_id=113, pid=25931[0m
25/07/19 13:46:03 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:46:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:46:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:46:06.926+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-19 13:43:55+00:00: auto__2025-07-19T13:43:55_f143e9e6, state:running, queued_at: 2025-07-19 13:44:17.963943+00:00. externally triggered: True> successful[0m
[[34m2025-07-19T13:46:06.927+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-19 13:43:55+00:00, run_id=auto__2025-07-19T13:43:55_f143e9e6, run_start_date=2025-07-19 13:45:08.745303+00:00, run_end_date=2025-07-19 13:46:06.927383+00:00, run_duration=58.18208, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-19 13:43:55+00:00, data_interval_end=2025-07-19 13:43:55+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/19 13:46:44 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:46:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:46:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:47:27 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:47:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:47:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:48:10 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:48:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:48:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:48:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:48:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:48:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:49:09.079+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:49:36 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:49:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:49:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:50:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:50:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:50:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:50:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:50:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:51:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:51:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:51:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:51:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:52:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:52:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:53:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:53:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:53:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:53:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:53:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:53:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:54:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:54:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:54:36.140+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 13:55:13 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:55:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:55:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:55:56 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:55:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:55:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:56:38 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:56:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:56:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:57:22 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:57:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:57:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:58:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:58:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:58:51 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:58:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:58:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 13:59:43 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 13:59:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 13:59:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T13:59:51.997+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:00:28 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:00:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:00:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:01:15 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:01:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:01:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:02:04 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:02:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:02:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:02:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:02:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:02:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:03:30 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:03:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:03:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:04:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:04:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:04:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:04:56 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:04:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:04:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:05:08.668+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:05:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:05:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:05:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:06:20 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:06:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:06:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:07:02 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:07:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:07:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:07:43 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:07:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:07:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:08:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:08:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:08:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:09:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:09:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:09:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:09:52 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:09:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:09:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:10:33 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:10:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:10:37.133+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:11:15 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:11:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:11:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:11:56 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:11:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:11:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:12:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:12:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:12:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:13:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:13:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:13:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:14:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:14:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:14:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:14:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:14:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:14:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:15:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:15:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:15:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:15:53.101+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:16:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:16:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:16:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:16:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:16:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:16:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:17:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:17:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:17:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:18:16 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:18:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:18:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:19:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:19:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:19:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:19:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:19:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:19:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:20:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:20:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:20:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:21:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:21:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:21:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:21:14.450+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:21:54 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:21:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:21:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:22:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:22:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:22:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:23:16 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:23:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:24:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:24:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:24:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:24:44 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:24:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:24:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:25:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:25:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:25:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:26:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:26:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:26:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:26:30.788+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:26:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:26:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:26:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:27:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:27:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:27:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:28:13 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:28:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:28:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:28:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:28:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:28:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:29:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:29:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:29:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:30:34 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:30:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:30:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:31:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:31:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:31:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:31:47.021+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:32:15 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:32:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:33:02 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:33:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:33:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:33:50 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:33:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:33:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:34:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:34:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:34:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:35:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:35:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:35:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:36:30 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:36:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:37:05.107+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:37:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:37:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:37:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:38:07 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:38:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:38:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:38:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:38:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:38:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:39:46 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:39:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:39:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:40:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:40:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:41:30 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:41:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:41:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:42:23 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:42:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:42:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:42:30.767+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:43:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:43:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:43:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:44:04 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:44:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:44:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:44:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:44:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:44:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:45:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:45:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:45:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:46:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:46:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:46:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Process DagFileProcessor83-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 500, in getOrCreate
    session = SparkSession(sc, options=self._options)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 589, in __init__
    jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py", line 1586, in __call__
    answer = self._gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 42913
[2025-07-19T14:46:50.233+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2025-07-19T14:46:51.586+0000] {manager.py:524} INFO - DAG ecommerce_etl is missing and will be deactivated.
[2025-07-19T14:46:51.612+0000] {manager.py:536} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-07-19T14:46:51.647+0000] {manager.py:540} INFO - Deleted DAG ecommerce_etl in serialized_dag table
25/07/19 14:47:31 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:47:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:47:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:47:47.337+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:48:19 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:48:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:49:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:49:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:49:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:49:58 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:49:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:49:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:51:00 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:51:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:51:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:51:55 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:51:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:51:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:52:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:52:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:52:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:53:05.769+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 14:53:33 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:53:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:54:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:54:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:54:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:55:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:55:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:56:03 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:56:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:56:51 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:56:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:56:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:57:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:57:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:57:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 14:58:31 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 14:58:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 14:58:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T14:58:39.880+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:03:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:03:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:03:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:04:05 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:04:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:04:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:04:54 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:04:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:04:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:05:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:05:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:05:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:06:38 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:06:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:07:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:07:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:07:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:07:59.778+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:08:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:08:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:08:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:09:06 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:09:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:09:53 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:09:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:09:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:10:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:10:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:10:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:11:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:11:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:11:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:12:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:12:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:12:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:13:18 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:13:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:13:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:13:24.603+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:14:10 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:14:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:14:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:15:02 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:15:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:15:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:15:50 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:15:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:15:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:16:42 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:16:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:17:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:17:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:17:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:18:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:18:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:18:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:18:41.939+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:19:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:19:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:19:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:20:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:20:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:20:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:21:08 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:21:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:21:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:22:00 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:22:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:22:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:22:51 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:22:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:22:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:23:43 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:23:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:23:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:23:50.924+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:24:34 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:24:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:24:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:25:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:25:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:25:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:26:16 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:26:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:26:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Process DagFileProcessor126-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 51890
25/07/19 15:27:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:27:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: /tmp/tmp2o26uv1n/connection6370145905420855487.info
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:380)
	at java.base/java.nio.file.Files.createFile(Files.java:658)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
	at java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)
	at java.base/java.nio.file.Files.createTempFile(Files.java:878)
	at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)
	at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2025-07-19T15:27:25.767+0000] {manager.py:524} INFO - DAG ecommerce_etl is missing and will be deactivated.
[2025-07-19T15:27:25.799+0000] {manager.py:536} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-07-19T15:27:25.826+0000] {manager.py:540} INFO - Deleted DAG ecommerce_etl in serialized_dag table
25/07/19 15:28:12 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:28:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:28:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:29:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:29:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:29:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:29:23.439+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:30:02 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:30:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:30:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:30:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:30:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:30:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:31:33 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:31:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:31:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:32:25 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:32:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:32:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:33:17 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:33:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:33:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:34:04 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:34:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:34:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:34:35.233+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:34:51 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:34:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:34:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:35:40 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:35:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:35:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:36:32 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:36:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:36:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:37:24 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:37:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:37:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:38:05 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:38:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:38:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:38:51 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:38:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:38:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:39:35 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:39:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:39:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:39:50.274+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:40:18 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:40:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:40:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:40:59 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:40:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:41:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:41:39 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:41:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:41:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:42:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:42:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:42:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:43:01 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:43:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:43:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:43:41 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:43:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:43:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:44:22 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:44:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:44:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:45:02 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:45:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:45:08.042+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:45:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:45:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:45:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:46:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:46:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:46:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:47:07 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:47:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:47:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:47:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:47:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:48:29 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:48:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:48:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:49:09 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:49:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:49:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:49:48 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:49:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:49:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-19T15:50:22.206+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/19 15:50:28 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:50:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:50:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/19 15:51:07 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/19 15:51:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 15:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-07-20T18:01:01.944+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-07-20T18:01:02.008+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2025-07-20T18:01:03.934+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2025-07-20T18:01:03.937+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-20T18:01:03.972+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 629[0m
[[34m2025-07-20T18:01:03.980+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-07-20T18:01:04.006+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-20 18:01:04 +0000] [628] [INFO] Starting gunicorn 23.0.0
[2025-07-20 18:01:04 +0000] [628] [INFO] Listening at: http://[::]:8793 (628)
[2025-07-20 18:01:04 +0000] [628] [INFO] Using worker: sync
[2025-07-20 18:01:04 +0000] [630] [INFO] Booting worker with pid: 630
[2025-07-20 18:01:04 +0000] [631] [INFO] Booting worker with pid: 631
[[34m2025-07-20T18:01:05.011+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-07-20T18:01:05.115+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
Process DagFileProcessor0-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 633
[[34m2025-07-20T18:01:39.934+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-20T18:01:07+00:00 [scheduled]>[0m
[[34m2025-07-20T18:01:39.936+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-20T18:01:39.939+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl manual__2025-07-20T18:01:07+00:00 [scheduled]>[0m
[[34m2025-07-20T18:01:39.998+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-20T18:01:07+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-20T18:01:40.003+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-20T18:01:07+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-20T18:01:40.038+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'manual__2025-07-20T18:01:07+00:00', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
25/07/20 18:02:21 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:02:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: /tmp/tmpizub6qjn/connection1659218526279212833.info
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:380)
	at java.base/java.nio.file.Files.createFile(Files.java:658)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
	at java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)
	at java.base/java.nio.file.Files.createTempFile(Files.java:878)
	at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)
	at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[[34m2025-07-20T18:02:33.428+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/20 18:02:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:02:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:02:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-20T18:02:57.995+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl manual__2025-07-20T18:01:07+00:00 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-20T18:03:27.685+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-20T18:03:28.993+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='manual__2025-07-20T18:01:07+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-20T18:03:29.530+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=manual__2025-07-20T18:01:07+00:00, map_index=-1, run_start_date=2025-07-20 18:02:58.831567+00:00, run_end_date=2025-07-20 18:03:27.199546+00:00, run_duration=28.367979, state=success, executor_state=success, try_number=1, max_tries=1, job_id=117, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-20 18:01:39.942333+00:00, queued_by_job_id=116, pid=904[0m
[[34m2025-07-20T18:03:29.605+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=629) last sent a heartbeat 109.62 seconds ago! Restarting it[0m
[[34m2025-07-20T18:03:29.637+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 629. PIDs of all processes in the group: [629][0m
[[34m2025-07-20T18:03:29.642+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 629[0m
[[34m2025-07-20T18:04:01.822+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=629, status='terminated', exitcode=0, started='18:01:06') (629) terminated with exit code 0[0m
[[34m2025-07-20T18:04:01.861+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 1278[0m
[[34m2025-07-20T18:04:02.111+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-20T18:04:05.223+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
Process DagFileProcessor0-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 1280
[[34m2025-07-20T18:04:37.657+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-20 18:01:07+00:00: manual__2025-07-20T18:01:07+00:00, state:running, queued_at: 2025-07-20 18:01:07.832360+00:00. externally triggered: True> successful[0m
[[34m2025-07-20T18:04:37.660+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-20 18:01:07+00:00, run_id=manual__2025-07-20T18:01:07+00:00, run_start_date=2025-07-20 18:01:39.390239+00:00, run_end_date=2025-07-20 18:04:37.659443+00:00, run_duration=178.269204, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-20 18:01:07+00:00, data_interval_end=2025-07-20 18:01:07+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
[[34m2025-07-20T18:04:39.955+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [scheduled]>[0m
[[34m2025-07-20T18:04:39.957+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-20T18:04:39.959+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [scheduled]>[0m
[[34m2025-07-20T18:04:40.071+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-20T18:01:08_5fcbe6b3', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-20T18:04:40.074+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-20T18:01:08_5fcbe6b3', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-20T18:04:40.100+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-20T18:01:08_5fcbe6b3', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
25/07/20 18:04:45 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:04:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: /tmp/tmp9xczkl8b/connection11435278034213216988.info
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:380)
	at java.base/java.nio.file.Files.createFile(Files.java:658)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
	at java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)
	at java.base/java.nio.file.Files.createTempFile(Files.java:878)
	at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)
	at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[[34m2025-07-20T18:05:27.167+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/20 18:05:49 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:05:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:05:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-20T18:06:00.298+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 1361[0m
Traceback (most recent call last):
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/bin/airflow", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/__main__.py", line 58, in main
    args.func(args)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 417, in task_run
    _dag = get_dag(args.subdir, args.dag_id, args.read_from_db)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/cli.py", line 235, in get_dag
    dagbag = DagBag(first_path)
             ^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 60, in <module>
    etl_task = PythonOperator(
               ^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/operators/python.py", line 220, in __init__
    super().__init__(**kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 988, in __init__
    self.weight_rule = validate_and_load_priority_weight_strategy(weight_rule)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/task/priority_strategy.py", line 135, in validate_and_load_priority_weight_strategy
    from airflow.serialization.serialized_objects import _get_registered_priority_weight_strategy
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/serialization/serialized_objects.py", line 53, in <module>
    from airflow.providers_manager import ProvidersManager
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/providers_manager.py", line 39, in <module>
    from airflow.hooks.filesystem import FSHook
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/hooks/filesystem.py", line 23, in <module>
    from airflow.hooks.base import BaseHook
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/hooks/base.py", line 36, in <module>
    class BaseHook(LoggingMixin):
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/hooks/base.py", line 106, in BaseHook
    @classmethod
     ^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 1361
[[34m2025-07-20T18:06:00.620+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-20T18:06:01.619+0000[0m] {[34msequential_executor.py:[0m81} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-20T18:01:08_5fcbe6b3', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py']' returned non-zero exit status 1..[0m
[[34m2025-07-20T18:06:01.862+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-20T18:01:08_5fcbe6b3', try_number=1, map_index=-1)[0m
[[34m2025-07-20T18:06:02.460+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=auto__2025-07-20T18:01:08_5fcbe6b3, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-20 18:04:39.960969+00:00, queued_by_job_id=116, pid=None[0m
[[34m2025-07-20T18:06:02.476+0000[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2025-07-20T18:06:03.211+0000[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
[[34m2025-07-20T18:06:03.746+0000[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=ecommerce_etl, task_id=run_etl, run_id=auto__2025-07-20T18:01:08_5fcbe6b3, execution_date=20250720T180108, start_date=, end_date=20250720T180603[0m
[[34m2025-07-20T18:06:04.330+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=1278) last sent a heartbeat 83.29 seconds ago! Restarting it[0m
[[34m2025-07-20T18:06:04.415+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1278. PIDs of all processes in the group: [1278][0m
[[34m2025-07-20T18:06:04.424+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1278[0m
[[34m2025-07-20T18:07:01.006+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=1278, status='terminated', exitcode=0, started='18:04:01') (1278) terminated with exit code 0[0m
[[34m2025-07-20T18:07:01.253+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 1868[0m
[[34m2025-07-20T18:07:01.524+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[[34m2025-07-20T18:07:02.587+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-07-20T18:07:03.252+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
Process DagFileProcessor0-Process:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
    return DagBag(file_path, include_examples=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 152, in __init__
    self.collect_dags(
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 318, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
    return parse(mod_name, filepath)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py", line 20, in <module>
    from src.etl.load import write_output
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/etl/load.py", line 7, in <module>
    spark = create_spark_session()
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/src/utils/spark_session.py", line 14, in create_spark_session
    .getOrCreate()
     ^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/sql/session.py", line 497, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 515, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
                                       ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/.venv/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.1/best-practices.html#reducing-dag-complexity, PID: 1869
25/07/20 18:07:47 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:07:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Exception in thread "main" java.nio.file.NoSuchFileException: /tmp/tmpjo840caq/connection1070299708142685570.info
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:380)
	at java.base/java.nio.file.Files.createFile(Files.java:658)
	at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
	at java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)
	at java.base/java.nio.file.Files.createTempFile(Files.java:878)
	at org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)
	at org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[[34m2025-07-20T18:08:03.950+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [scheduled]>[0m
[[34m2025-07-20T18:08:03.952+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ecommerce_etl has 0/16 running and queued tasks[0m
[[34m2025-07-20T18:08:03.955+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [scheduled]>[0m
[[34m2025-07-20T18:08:03.977+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-20T18:01:08_5fcbe6b3', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-20T18:08:03.980+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-20T18:01:08_5fcbe6b3', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-20T18:08:04.002+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ecommerce_etl', 'run_etl', 'auto__2025-07-20T18:01:08_5fcbe6b3', '--local', '--subdir', 'DAGS_FOLDER/ecommerce_etl_dag.py'][0m
[[34m2025-07-20T18:08:53.480+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /mnt/c/Users/Shivam Gupta/OneDrive/Documents/Shivam_Developement/DATA_ENGINEERING/data_engineering_project/batch-etl-pyspark/airflow/dags/ecommerce_etl_dag.py[0m
25/07/20 18:09:05 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:09:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Creating SparkSession with appName: EcommerceSalesAnalytics and master: local[*]
SparkSession created successfully.
[[34m2025-07-20T18:09:14.281+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ecommerce_etl.run_etl auto__2025-07-20T18:01:08_5fcbe6b3 [queued]> on host LAPTOP-2CRA05LG.[0m
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                          (0 + 1) / 1]                                                                                [[34m2025-07-20T18:09:46.028+0000[0m] {[34mclientserver.py:[0m543} INFO[0m - Closing down clientserver connection[0m
[[34m2025-07-20T18:09:47.757+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ecommerce_etl', task_id='run_etl', run_id='auto__2025-07-20T18:01:08_5fcbe6b3', try_number=2, map_index=-1)[0m
[[34m2025-07-20T18:09:48.917+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ecommerce_etl, task_id=run_etl, run_id=auto__2025-07-20T18:01:08_5fcbe6b3, map_index=-1, run_start_date=2025-07-20 18:09:15.390801+00:00, run_end_date=2025-07-20 18:09:45.580426+00:00, run_duration=30.189625, state=success, executor_state=success, try_number=2, max_tries=1, job_id=118, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-20 18:08:03.957977+00:00, queued_by_job_id=116, pid=2115[0m
[[34m2025-07-20T18:09:48.968+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=1868) last sent a heartbeat 103.47 seconds ago! Restarting it[0m
[[34m2025-07-20T18:09:49.019+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 1868. PIDs of all processes in the group: [1868][0m
[[34m2025-07-20T18:09:49.025+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 1868[0m
[[34m2025-07-20T18:10:09.319+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=1868, status='terminated', exitcode=0, started='18:06:56') (1868) terminated with exit code 0[0m
[[34m2025-07-20T18:10:09.357+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 2310[0m
[[34m2025-07-20T18:10:09.481+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-07-20T18:10:10.531+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
25/07/20 18:10:26 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:10:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:10:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-20T18:10:37.345+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ecommerce_etl @ 2025-07-20 18:01:08+00:00: auto__2025-07-20T18:01:08_5fcbe6b3, state:running, queued_at: 2025-07-20 18:02:35.031573+00:00. externally triggered: True> successful[0m
[[34m2025-07-20T18:10:37.356+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ecommerce_etl, execution_date=2025-07-20 18:01:08+00:00, run_id=auto__2025-07-20T18:01:08_5fcbe6b3, run_start_date=2025-07-20 18:04:39.635286+00:00, run_end_date=2025-07-20 18:10:37.354919+00:00, run_duration=357.719633, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-20 18:01:08+00:00, data_interval_end=2025-07-20 18:01:08+00:00, dag_hash=ddf3096a82af1521d090f8219971a8cb[0m
25/07/20 18:11:22 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:11:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:11:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/20 18:12:18 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:12:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:12:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[[34m2025-07-20T18:12:27.297+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
25/07/20 18:13:14 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:13:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:13:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/07/20 18:14:11 WARN Utils: Your hostname, LAPTOP-2CRA05LG resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/07/20 18:14:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/20 18:14:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
