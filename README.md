# ğŸ› ï¸ PySpark E-Commerce ETL Project

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![CI](https://github.com/<your-username>/<repo-name>/actions/workflows/ci.yml/badge.svg)
[![codecov](https://codecov.io/gh/<your-username>/<repo-name>/branch/main/graph/badge.svg)](https://codecov.io/gh/<your-username>/<repo-name>)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)
![Security](https://img.shields.io/badge/security-disclosures-important.svg)

> A production-grade, **portfolio-ready PySpark batch ETL pipeline** for e-commerce analytics  
> Featuring modular ETL layers, CI/CD with GitHub Actions, Apache Airflow orchestration, testing, logging, notebook output, and Power BI visualization.

---

## ğŸ“¦ Project Structure

```text
batch-etl-pyspark/
â”‚
â”œâ”€â”€ .github/workflows/ci.yml        # âœ… GitHub Actions (CI/CD)
â”œâ”€â”€ airflow/                        # Airflow configs + DB
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ ecommerce_etl_dag.py    # Airflow DAG
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                      # ğŸ“¥ Raw CSVs (products, customers, orders)
â”‚   â””â”€â”€ processed/                  # ğŸ“¤ Output after ETL
â”œâ”€â”€ logs/                           # ğŸªµ Logs from scheduler, webserver, dag, etc.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ dev_etl_demo.ipynb          # ğŸ““ Papermill-executed analysis notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ extract.py              # extract_*() logic
â”‚   â”‚   â”œâ”€â”€ transform.py            # clean + join + derive metrics
â”‚   â”‚   â””â”€â”€ load.py                 # write to CSV/parquet
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ spark_session.py        # SparkSession builder
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py             # âœ… Unit test: extract
â”‚   â”œâ”€â”€ test_transform.py           # âœ… Unit test: transform
â”‚   â”œâ”€â”€ test_load.py                # âœ… Unit test: load
â”‚   â””â”€â”€ test_sales_metrics.py       # Optional metrics tests
â”œâ”€â”€ .env                            # Configurable paths/secrets
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE                         # MIT License
â”œâ”€â”€ README.md                       # ğŸ™Œ You're reading it
â”œâ”€â”€ requirements.txt                # All packages for local dev
â”œâ”€â”€ full_project_runner.sh          # âš™ï¸ One-click launch script (Airflow + Notebook + Power BI)
â””â”€â”€ pyproject.toml (optional)

âš™ï¸ Features

    âœ… Modular PySpark-based ETL pipeline

    âœ… Airflow DAG orchestration + scheduling

    âœ… Papermill notebook execution (automated)

    âœ… Power BI integration (CSV â†’ Desktop auto-launch)

    âœ… Environment management via .env

    âœ… Unit testing with pytest (extraction, transformation, loading)

    âœ… Logging: airflow, scheduler, notebook logs all tracked

    âœ… GitHub Actions-based CI (via ci.yml)

    âœ… One-command launcher (full_project_runner.sh) for demo/presentation

    âœ… Production-style repo structure â€” ready for interviews or GitHub Pages

ğŸš€ Getting Started
1ï¸âƒ£ Clone this Repository

git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

2ï¸âƒ£ Set up Virtual Environment

python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

3ï¸âƒ£ Create .env File

OUTPUT_PATH=./data/processed/

4ï¸âƒ£ Run Locally via PySpark

python src/main.py

5ï¸âƒ£ Or Run Full Pipeline via Airflow

chmod +x full_project_runner.sh
./full_project_runner.sh

This will:

    Start Airflow scheduler + webserver

    Trigger the DAG

    Execute the Jupyter notebook

    Export CSV

    Auto-open Power BI with the result âœ…

ğŸ“ˆ Sample Output
Metric	Value
Total Orders	xxx
Revenue	â‚¹xxx.xx
Top Customers	âœ… ranked
Category Trend	ğŸ“ˆ chart

ğŸ“ Output is saved to data/processed/processed_output.csv
ğŸ“ Notebook output: notebooks/dev_etl_demo_<date>.ipynb
ğŸ§ª Testing

pytest tests/

Includes:

    test_extract.py â†’ Validates loading raw CSVs

    test_transform.py â†’ Ensures derived metrics work

    test_load.py â†’ Validates CSV write

    test_sales_metrics.py â†’ Optional: business logic

ğŸ” CI/CD Integration

âœ… GitHub Actions included via:

.github/workflows/ci.yml

Every push to main:

    Installs Python + dependencies

    Runs full test suite via pytest

    Fails build if any test fails

ğŸ” Security & Secrets

    .env manages all paths

    Add .env.example for team usage

    Optional encryption via git-crypt or sops

ğŸ§³ Project Resume Summary (Use for Portfolio)

    âœ… A complete, reproducible ETL + Analytics pipeline using PySpark
    ğŸ› ï¸ Designed for real-world batch processing
    ğŸ“Š Integrated with Airflow, CI/CD, Power BI, and unit testing
    ğŸ¯ Ready to showcase in interviews and GitHub profile

ğŸ§  Advanced Tips (Optional)

    Add docs/ folder + mkdocs.yml â†’ for GitHub Pages site

    Add .deb or .whl packaging only if publishing to PyPI (skip otherwise)

    Use GitHub Secrets + dotenv-linter for secure .env handling

ğŸ“Œ GitHub First-Time Setup (manual once)

git init
git remote add origin https://github.com/<your-username>/<repo-name>.git
git add .
git commit -m "ğŸš€ Final production-ready ETL project"
git push -u origin main

# Optional: tag version
git tag v1.0.0
git push origin v1.0.0

ğŸ“ License

This project is licensed under the MIT License â€” see LICENSE for details.
ğŸ’¬ Questions?

Feel free to open an issue, star the repo, or reach out on LinkedIn!

    Built with ğŸ’™ using PySpark + Airflow + VSCode + GitHub Actions
    #DataEngineering #PortfolioProject #ETL #CI/CD #Papermill #Airflow"# BatchEtlPyspark-20250723" 
